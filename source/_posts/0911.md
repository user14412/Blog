---
title: 911
date: 2025-09-11 20:32:31
tags:
---
[TOC]

# 1. 从零开始运行一个精调一个大模型demo

## 1.1 前期准备

**模型地址**

- [ ] 中文Llama2模型huggingface：[LinkSoul/Chinese-Llama-2-7b at main](https://huggingface.co/LinkSoul/Chinese-Llama-2-7b/tree/main)

- [ ] 中文Llama2模型github：[LinkSoul-AI/Chinese-Llama-2-7b: 开源社区第一个能下载、能运行的中文 LLaMA2 模型！](https://github.com/LinkSoul-AI/Chinese-Llama-2-7b)

**精调数据集and精调脚本参考来源**

- [ ] 介绍文章：[ 大模型为猫娘而生！NekoQA-10K: 面向猫娘语言建模的对话数据集，登上顶刊ZHIHU - 知乎](https://zhuanlan.zhihu.com/p/1934983798233231689)

- [ ] 数据集**NekoQA-10K**：[liumindmind/NekoQA-10K · Datasets at Hugging Face](https://huggingface.co/datasets/liumindmind/NekoQA-10K)

- [ ] [mindsRiverPonder/LLM-practice: 一些奇思妙想，使用大语言模型制作各种小应用](https://github.com/mindsRiverPonder/LLM-practice)

**GPU租用**

- [ ] Autodl平台：[AutoDL算力云 | 弹性、好用、省钱，GPU算力零售价格新标杆](https://autodl.com/home)

## 1.2 操作步骤

### 1.2.1 本地部署并测试llama2预训练模型

租一台云服务器，配置环境

```shell
# 初始化conda
conda init
source ~/.bashrc
# 创建conda环境
conda create -n pytorch python=3.10
```

**确保GPU可用**（下面的程序输出True）

```python
import torch
print(torch.cuda.is_available())
print(torch.__version__)
```

#### 部署方式

- [ ] 方案一，**手动部署**，直接在huggingface/github/网盘下载模型到本地磁盘，下方的model_path填本地路径

- [ ] 方案二，**API部署**，通过transformer库API部署，model_path填模型的名称

#### 问题一、网络问题：

**huggingface.co无法访问**==>换镜像站

```shell
export HF_ENDPOINT=https://hf-mirror.com
python your_script.py
```

#### 问题二、存储问题：

**模型太大**导致默认下载路径（系统盘）装不下==>换方案一本地下载到数据盘，要不就加配置

系统盘太小导致后续下载**python库把系统盘撑爆**==>省掉推理优化的库，一般做的都很大；实在不行加配置

#### 问题三、如何把本地的模型上传到云服务器上

推荐公网网盘：[AutoDL帮助文档](https://autodl.com/docs/netdisk/)

#### 测试脚本

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer

model_path = "/root/autodl-tmp/Chinese-Llama-2-7b"

tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
model = AutoModelForCausalLM.from_pretrained(model_path).half().cuda()
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

instruction = """[INST] <<SYS>>\\nYou are NekoQA-llama3.2-3B.ipynb helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

            If NekoQA-llama3.2-3B.ipynb question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to NekoQA-llama3.2-3B.ipynb question, please don't share false information.\\n<</SYS>>\\n\\n{} [/INST]"""

prompt = instruction.format("用中文回答，如果你统治了世界，你会欺负我吗？")
generate_ids = model.generate(tokenizer(prompt, return_tensors='pt').input_ids.cuda(), max_new_tokens=4096, streamer=streamer)
```

如果成功部署与训练模型，模型会输出相应回答

### 1.2.2 指令精调

准备好预训练模型和精调指令集，在脚本相应位置替换为实际路径

运行精调脚本，GPU将开始训练。这里平均4分钟完成一轮训练，共150轮，大约10小时后完成训练

**精调脚本**

精调脚本包含：

- [ ] 精调前：各种配置和加载
- [ ] 精调训练过程
- [ ] 精调后：保存精调后的模型、测试精调后的模型

```python
import os
import torch
from datasets import load_dataset
from huggingface_hub import login
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DataCollatorForSeq2Seq,
    TextStreamer,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from transformers import TrainingArguments

def main():
    # #################################################################################
    # ## 1. 全局配置
    # #################################################################################

    # --- 模型和数据集路径 ---
    # TODO: 请确认模型路径和数据集路径是否正确
    model_path = "/root/autodl-tmp/Chinese-Llama-2-7b"
    dataset_path = "/root/proj/NekoQA-10K.json"

    # --- Hugging Face 登录 ---
    # 如果模型是私有的，或者需要从Hugging Face Hub下载，请取消注释并填入您的token
    # hf_token = "YOUR_HF_TOKEN"
    # login(hf_token)

    # --- 训练参数 ---
    max_seq_length = 2048
    load_in_4bit = True  # 对于7B模型，推荐开启4bit量化以节省显存
    batch_size = 2  # 批处理大小
    grad_accum_steps = 16  # 梯度累积步数
    max_steps = 150  # 最大训练步数
    learning_rate = 2e-4

    # --- LoRA 配置 ---
    lora_r = 32
    lora_alpha = 32
    lora_target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]

    # --- 环境配置 (可选) ---
    # os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
    # os.environ["CUDA_VISIBLE_DEVICES"] = "1"
    # os.environ["NCCL_P2P_DISABLE"] = "1"
    # os.environ["NCCL_IB_DISABLE"] = "1"

    # #################################################################################
    # ## 2. 加载模型和分词器
    # #################################################################################
    print(">>> 开始加载模型和分词器...")

    # 加载分词器
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    # 配置4-bit量化
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    # 加载模型
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=quantization_config if load_in_4bit else None,
        device_map="auto",  # 自动将模型分发到可用设备（GPU）
        torch_dtype=torch.bfloat16,
        # low_cpu_mem_usage=True, # <--- 添加这一行以减少CPU内存使用
    )

    # 修复Llama-2分词器pad_token的问题
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = model.config.eos_token_id

    print(">>> 模型和分词器加载完毕。")

    # #################################################################################
    # ## 3. 配置PEFT (LoRA)
    # #################################################################################
    print(">>> 配置PEFT (LoRA)...")

    # 为k-bit训练准备模型
    model = prepare_model_for_kbit_training(model)

    # 创建LoRA配置
    peft_config = LoraConfig(
        r=lora_r,
        lora_alpha=lora_alpha,
        target_modules=lora_target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    # 应用PEFT到模型
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()  # 打印可训练参数信息

    print(">>> PEFT (LoRA) 配置完毕。")

    # #################################################################################
    # ## 4. 加载和预处理数据集
    # #################################################################################
    print(">>> 开始加载和处理数据集...")
    dataset = load_dataset("json", data_files=dataset_path, split="train")

    def formatting_prompts_func(examples):
        # TODO: 请根据您的数据集字段修改 "question" 和 "answer"
        conversations = [
            [
                {"role": "user", "content": inst},
                {"role": "assistant", "content": out}
            ]
            for inst, out in zip(examples["instruction"], examples["output"])
        ]
        # 对于Llama-2，训练时不需要添加起始符<s>，但需要确保结尾是eos_token
        texts = [
            tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) + tokenizer.eos_token
            for convo in conversations
        ]
        return {"text": texts}

    dataset = dataset.map(formatting_prompts_func, batched=True)
    print(">>> 数据集加载和处理完毕。")

    # #################################################################################
    # ## 5. 配置和启动训练
    # #################################################################################
    print(">>> 配置训练器...")
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        packing=True,  # 将多个短样本打包成一个长样本，提升效率
        peft_config=peft_config,
        args=TrainingArguments( # <--- 这里从 SFTConfig 改为 TrainingArguments
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=grad_accum_steps,
            warmup_steps=5,
            max_steps=max_steps,
            learning_rate=learning_rate,
            logging_steps=1,
            optim="paged_adamw_8bit",
            weight_decay=0.01,
            lr_scheduler_type="linear",
            seed=3407,
            output_dir="outputs",
            report_to="none",
        ),
    )


    print(">>> 开始模型训练...")
    trainer.train()
    print(">>> 模型训练完毕。")

    # #################################################################################
    # ## 6. 保存模型
    # #################################################################################
    output_model_path = "Chinese-Llama-2-7b-finetuned"
    print(f">>> 保存微调后的模型到 {output_model_path}...")
    trainer.save_model(output_model_path)
    # 单独保存分词器
    tokenizer.save_pretrained(output_model_path)
    print(">>> 模型保存完毕。")

    # #################################################################################
    # ## 7. 运行推理测试
    # #################################################################################
    print("\n>>> 开始运行推理测试...")

    # 清理显存并加载用于推理的模型
    del model, trainer
    torch.cuda.empty_cache()

    # 从保存的路径加载PEFT模型进行推理
    from peft import PeftModel
    base_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    model = PeftModel.from_pretrained(base_model, output_model_path)
    tokenizer = AutoTokenizer.from_pretrained(output_model_path)

    messages = [
        {"role": "user", "content": "你好！你是一个什么模型？"},
    ]
    inputs = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    ).to("cuda")

    text_streamer = TextStreamer(tokenizer, skip_prompt=True)
    _ = model.generate(
        input_ids=inputs,
        streamer=text_streamer,
        max_new_tokens=512,
        use_cache=True,
        temperature=1.0,
        min_p=0.1
    )
    print("\n>>> 推理测试完成。")


if __name__ == "__main__":
    main()
```

# 2. 软件分析lesson1

## 2.1 人文教育

以下内容作为钢印烙印在脑子里，作为接下来少则三年、多则一生相信与秉行的**==原则==**

**不要浮躁**

- [ ] 虚荣感and安全感成为社会文化长期以来塑造的人们要去用生命追求的东西（价值观/评价体系）
- [ ] 这种现象表现在，大家普遍追求毕业后获得一份还算体面的高薪的工作、结婚、生子、给父母养老就满足了；But，谁规定了我们的人生一定要这样活？人只活一次，每个人都是有个性的，所以一定要**认识自己**，只有认识自己，才能活出自己的人生。
- [ ] 我认为这样才算有意义的人生，因为"生娃放羊、放羊生娃"式地追求安全感和虚荣感的预制人生毫无意义；而之所以大多数人，包括令人尊敬的师长他们也会陷入这种追求，是因为这种社会文化的强大（电视、教育、口口相传形成的社会文化）；为什么这种人生毫无意义？因为"吃200一顿比吃20一顿的生活品质好"这种观念不是先验的，而是别人（社会文化）告诉你这种生活方式是好的、是有品质的，而这只是别人告诉你的，而并非你自己经过思考得出、自己这么认为的，所以追求这些别人告诉你好的东西本身是和一般理解的人生的意义相悖的。
- [ ] 这种现象还表现在，大家普遍喜欢和别人比（虚荣感），按照社会评价体系，组里总有比自己出色的人。这样就会让我们否定自己、**失掉自信**。
- [ ] 认识到社会价值观中和自己价值观重合的部分，既不被社会价值观带偏，也不盲目地看不到社会价值观的有价值的部分。是一种很高级的认知境界。

**认识自己**

- [ ] 认识自己是我们要花一生的时间去做的课题
- [ ] **健康和快乐是人生唯二的意义**

**重拾自信**

- [ ] 不论何时，保持自信
- [ ] 保持自信是保持快乐的重要组成部分（三感之一），保持自信可以显著增加你的生命质量
- [ ] 自信是一种需要持续精进修炼的心态
- [ ] **永远不要否定自己**：遇到比自己出色的人：一件事情做的不如别人，不要否定自己这个人；只是这一个课题而已，换一个我也许行；
- [ ] **永远不要放弃自己**，觉得自己反正也比不过别人，我就是这样一个不如别人的人，就这样算了，找到一份还算高薪的工作结婚生子养老就很满意了（人只活一次，放弃了自己，放弃认识自己，掉入了社会预制价值，这一生会不值得过）
- [ ] 保持自信的同时，在具体问题上客观分析具体问题利弊局面，是一种高级的境界

## 2.2 软件分析

- [ ] 什么是PL研究？软件分析关注什么？

- [ ] 大米定律（Rice Theorem）==>不可能同时达到Sound和Complete
- [ ] **Sound > Truth > Complete**
  - [ ] Sound = 误报 = False Positive（静态分析一般希望的）
  - [ ] Complete = 漏报 = False Negative
- [ ] 程序分析案例Abstraction + up-Approximation